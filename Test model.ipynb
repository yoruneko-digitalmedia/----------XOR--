{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing dataset from dataset.csv\n",
      "Model loaded from best_val_model.npz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class ShallowNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.t = 0\n",
    "        \n",
    "        self.W1, self.b1, self.W2, self.b2 = self.initialize_weights()\n",
    "        self.initialize_adam_parameters()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "\n",
    "        W1 = cp.random.randn(self.hidden_size, self.input_size) * 0.01\n",
    "        W2 = cp.random.randn(self.output_size, self.input_size) * 0.01\n",
    "        \n",
    "        b1 = cp.zeros((self.hidden_size, 1))\n",
    "        b2 = cp.zeros((self.output_size, 1))\n",
    "        return W1, b1, W2, b2\n",
    "    \n",
    "    def initialize_adam_parameters(self):\n",
    "        self.mW1, self.vW1 = cp.zeros_like(self.W1), cp.zeros_like(self.W1)\n",
    "        self.mb1, self.vb1 = cp.zeros_like(self.b1), cp.zeros_like(self.b1)\n",
    "        self.mW2, self.vW2 = cp.zeros_like(self.W2), cp.zeros_like(self.W2)\n",
    "        self.mb2, self.vb2 = cp.zeros_like(self.b2), cp.zeros_like(self.b2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.z1 = cp.dot(self.W1, x) + self.b1\n",
    "        self.a1 = cp.tanh(self.z1)\n",
    "        self.z2 = cp.dot(self.W2, self.a1) + self.b2\n",
    "        self.output = cp.tanh(self.z2)\n",
    "        return self.output\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.forward(X)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x, y):\n",
    "        m = x.shape[1]\n",
    "        dz2 = self.output - y\n",
    "        dW2 = (1/m) * cp.dot(dz2, self.a1.T)\n",
    "        db2 = (1/m) * cp.sum(dz2, axis=1, keepdims=True)\n",
    "        dz1 = cp.dot(self.W2.T, dz2) * (1 - cp.power(self.a1, 2))\n",
    "        dW1 = (1/m) * cp.dot(dz1, x.T)\n",
    "        db1 = (1/m) * cp.sum(dz1, axis=1, keepdims=True)\n",
    "        self.update_weights(dW1, db1, dW2, db2)\n",
    "\n",
    "    def update_weights(self, dW1, db1, dW2, db2):\n",
    "        self.t += 1\n",
    "        def adam_update(m, v, grad, beta1, beta2, epsilon, t):\n",
    "            m = beta1 * m + (1 - beta1) * grad\n",
    "            v = beta2 * v + (1 - beta2) * cp.power(grad, 2)\n",
    "            m_hat = m / (1 - cp.power(beta1, t))\n",
    "            v_hat = v / (1 - cp.power(beta2, t))\n",
    "            return m, v, m_hat / (cp.sqrt(v_hat) + epsilon)\n",
    "        \n",
    "        self.mW1, self.vW1, mW1_hat = adam_update(self.mW1, self.vW1, dW1, self.beta1, self.beta2, self.epsilon, self.t)\n",
    "        self.mb1, self.vb1, mb1_hat = adam_update(self.mb1, self.vb1, db1, self.beta1, self.beta2, self.epsilon, self.t)\n",
    "        self.mW2, self.vW2, mW2_hat = adam_update(self.mW2, self.vW2, dW2, self.beta1, self.beta2, self.epsilon, self.t)\n",
    "        self.mb2, self.vb2, mb2_hat = adam_update(self.mb2, self.vb2, db2, self.beta1, self.beta2, self.epsilon, self.t)\n",
    "\n",
    "        self.W1 -= self.learning_rate * mW1_hat\n",
    "        self.b1 -= self.learning_rate * mb1_hat\n",
    "        self.W2 -= self.learning_rate * mW2_hat\n",
    "        self.b2 -= self.learning_rate * mb2_hat\n",
    "            \n",
    "    def train(self, X, y, X_val, y_val, epochs=1, batch_size=1024, patience=10):\n",
    "        losses, val_losses = [], []\n",
    "        best_val_loss, best_weights = float('inf'), None\n",
    "        epochs_no_improve, stopped_epoch = 0, 0\n",
    "        num_batches = (X.shape[1] + batch_size - 1) // batch_size\n",
    "        total_steps = epochs * num_batches\n",
    "\n",
    "        with tqdm(total=total_steps, desc=\"Training Progress\") as bar:\n",
    "            for epoch in range(epochs):\n",
    "                for i in range(0, X.shape[1], batch_size):\n",
    "                    x_batch = X[:, i:i + batch_size]\n",
    "                    y_batch = y[:, i:i + batch_size]\n",
    "                    self.forward(x_batch)\n",
    "                    self.backward(x_batch, y_batch)\n",
    "                    bar.update(1)\n",
    "\n",
    "                y_pred_train = self.predict(X)\n",
    "                loss = mean_squared_error(y.get(), y_pred_train.get())\n",
    "                losses.append(loss)\n",
    "\n",
    "                y_pred_val = self.predict(X_val)\n",
    "                val_loss = mean_squared_error(y_val.get(), y_pred_val.get())\n",
    "                val_losses.append(val_loss)\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss, best_weights = val_loss, (self.W1.copy(), self.b1.copy(), self.W2.copy(), self.b2.copy())\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                \n",
    "                if epochs_no_improve >= patience:\n",
    "                    stopped_epoch = epoch\n",
    "                    break\n",
    "\n",
    "            if best_weights is not None:\n",
    "                self.W1, self.b1, self.W2, self.b2 = best_weights\n",
    "            \n",
    "            return losses, val_losses, stopped_epoch\n",
    "    def __str__(self):\n",
    "        return f'\\nW1={self.W1}\\nb1={self.b1}\\nW2={self.W2}\\nb2={self.b2}'\n",
    "    \n",
    "    def save(self, file_path):\n",
    "        np.savez(file_path, \n",
    "                    W1=self.W1.get(), \n",
    "                    b1=self.b1.get(), \n",
    "                    W2=self.W2.get(), \n",
    "                    b2=self.b2.get())\n",
    "        print(f\"Model saved to {file_path}\")\n",
    "\n",
    "    def load(self, file_path):\n",
    "        data = np.load(file_path)\n",
    "        self.W1 = cp.array(data['W1'])\n",
    "        self.b1 = cp.array(data['b1'])\n",
    "        self.W2 = cp.array(data['W2'])\n",
    "        self.b2 = cp.array(data['b2'])\n",
    "        print(f\"Model loaded from {file_path}\")\n",
    "def generate_data(num_samples=10000):\n",
    "    # np.random.seed(42)\n",
    "    # 生成隨機數據，分別從兩個區間選擇\n",
    "    X = np.zeros((num_samples, 2))\n",
    "    for i in range(num_samples):\n",
    "        for j in range(2):\n",
    "            X[i, j] = np.random.choice([np.random.uniform(-0.5, 0.2), np.random.uniform(0.8, 1.5)])\n",
    "    # 將數值轉換為0或1\n",
    "    X_binarized = (X > 0.5).astype(int)\n",
    "    # 計算 XOR\n",
    "    y = np.bitwise_xor(X_binarized[:, 0], X_binarized[:, 1]).reshape(-1, 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def check_and_save_data(csv_path, num_samples):\n",
    "    if os.path.exists(csv_path):\n",
    "        print(f\"Loading existing dataset from {csv_path}\")\n",
    "        data = pd.read_csv(csv_path)\n",
    "        X = data[['x1', 'x2']].values\n",
    "        y = data['y'].values.reshape(-1, 1)\n",
    "    else:\n",
    "        print(f\"No existing dataset found. Generating new data...\")\n",
    "        X, y = generate_data(num_samples)\n",
    "        pd.DataFrame({'x1': X[:, 0], 'x2': X[:, 1], 'y': y.flatten()}).to_csv(csv_path, index=False)\n",
    "        print(f\"Dataset saved to {csv_path}\")\n",
    "    return X, y\n",
    "\n",
    "class Config:\n",
    "    EPOCHS = 10\n",
    "    BATCH_SIZE = pow(2, 4)\n",
    "    NUM_SAMPLES = pow(2, 14)\n",
    "    LEARNING_RATE = 0.001\n",
    "    MIN_RANGE = 2\n",
    "    MAX_RANGE = 2\n",
    "    HIDDEN_SIZES = range(MIN_RANGE, MAX_RANGE + 1)\n",
    "    ROUNDS = 5\n",
    "    PATIENCE = int(EPOCHS*0.5)\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"Config:\\n\"\n",
    "            f\"  LEARNING_RATE={self.LEARNING_RATE}\\n\"\n",
    "            f\"  EPOCHS={self.EPOCHS}\\n\"\n",
    "            f\"  BATCH_SIZE={self.BATCH_SIZE}\\n\"\n",
    "            f\"  NUM_SAMPLES={self.NUM_SAMPLES}\\n\"\n",
    "            f\"  MIN_RANGE={self.MIN_RANGE}\\n\"\n",
    "            f\"  MAX_RANGE={self.MAX_RANGE}\\n\"\n",
    "            f\"  HIDDEN_SIZES={list(self.HIDDEN_SIZES)}\\n\"\n",
    "            f\"  ROUNDS={self.ROUNDS}\\n\"\n",
    "            f\"  PATIENCE={self.PATIENCE}\\n\"\n",
    "            )\n",
    "\n",
    "config = Config()\n",
    "X, y = check_and_save_data('dataset.csv', config.NUM_SAMPLES)\n",
    "train_size = int(0.8 * X.shape[0])\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "scaler_X_0, scaler_X_1 = StandardScaler(), StandardScaler()\n",
    "X_train_0 = scaler_X_0.fit_transform(X_train[:, 0].reshape(-1, 1))\n",
    "X_train_1 = scaler_X_1.fit_transform(X_train[:, 1].reshape(-1, 1))\n",
    "X_train = np.hstack((X_train_0, X_train_1))\n",
    "\n",
    "X_test_0 = scaler_X_0.transform(X_test[:, 0].reshape(-1, 1))\n",
    "X_test_1 = scaler_X_1.transform(X_test[:, 1].reshape(-1, 1))\n",
    "X_test = np.hstack((X_test_0, X_test_1))\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# 保存標準化器\n",
    "\n",
    "np.save('scaler_x0_mean.npy', scaler_X_0.mean_)\n",
    "np.save('scaler_x0_scale.npy', scaler_X_0.scale_)\n",
    "np.save('scaler_x1_mean.npy', scaler_X_1.mean_)\n",
    "np.save('scaler_x1_scale.npy', scaler_X_1.scale_)\n",
    "\n",
    "X_train_T = X_train.T\n",
    "y_train_T = y_train.reshape(1, -1)\n",
    "\n",
    "X_train = cp.array(X_train.T, dtype=cp.float32)\n",
    "X_test = cp.array(X_test.T, dtype=cp.float32)\n",
    "y_train = cp.array(y_train.T, dtype=cp.float32)\n",
    "y_test = cp.array(y_test.T, dtype=cp.float32)\n",
    "\n",
    "nn = ShallowNeuralNetwork(input_size=2, hidden_size=10, output_size=1, learning_rate=0.01)\n",
    "nn.load('best_val_model.npz')\n",
    "\n",
    "def calculate(input_x1, input_x2):\n",
    "    scaler_X_0 = StandardScaler()\n",
    "    scaler_X_1 = StandardScaler()\n",
    "    scaler_X_0.mean_ = np.load('scaler_x0_mean.npy')\n",
    "    scaler_X_0.scale_ = np.load('scaler_x0_scale.npy')\n",
    "    scaler_X_1.mean_ = np.load('scaler_x1_mean.npy')\n",
    "    scaler_X_1.scale_ = np.load('scaler_x1_scale.npy')\n",
    "\n",
    "    input_x1_scaled = scaler_X_0.transform(np.array([[input_x1]]))\n",
    "    input_x2_scaled = scaler_X_1.transform(np.array([[input_x2]]))\n",
    "    inputs = cp.array([[input_x1_scaled], [input_x2_scaled]], dtype=cp.float32).reshape(2, 1)\n",
    "\n",
    "    nn.forward(inputs)\n",
    "    output = nn.output.get()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.01\txor\t1.38\t= 0.002226 ~ 0\t(ideal: 0)\n",
      "0.09\txor\t1.24\t= 0.756392 ~ 1\t(ideal: 1)\n",
      "1.17\txor\t0.93\t= 0.002040 ~ 0\t(ideal: 0)\n",
      "1.46\txor\t1.05\t= 0.047598 ~ 0\t(ideal: 0)\n",
      "1.40\txor\t-0.02\t= 0.759916 ~ 1\t(ideal: 1)\n",
      "0.84\txor\t0.99\t= 0.002040 ~ 0\t(ideal: 0)\n",
      "-0.16\txor\t1.38\t= 0.756392 ~ 1\t(ideal: 1)\n",
      "-0.09\txor\t-0.07\t= 0.002040 ~ 0\t(ideal: 0)\n",
      "0.00\txor\t0.85\t= 0.756392 ~ 1\t(ideal: 1)\n",
      "-0.36\txor\t-0.14\t= 0.002040 ~ 0\t(ideal: 0)\n",
      "0.10\txor\t1.48\t= 0.756392 ~ 1\t(ideal: 1)\n",
      "-0.39\txor\t1.03\t= 0.756392 ~ 1\t(ideal: 1)\n",
      "-0.49\txor\t0.92\t= 0.756392 ~ 1\t(ideal: 1)\n",
      "0.10\txor\t1.39\t= 0.756392 ~ 1\t(ideal: 1)\n",
      "0.02\txor\t1.33\t= 0.756392 ~ 1\t(ideal: 1)\n",
      "-0.27\txor\t-0.05\t= 0.002040 ~ 0\t(ideal: 0)\n",
      "1.44\txor\t-0.20\t= 0.759916 ~ 1\t(ideal: 1)\n",
      "1.25\txor\t1.27\t= 0.002040 ~ 0\t(ideal: 0)\n",
      "1.40\txor\t0.90\t= 0.756013 ~ 1\t(ideal: 0)\n",
      "-0.40\txor\t0.08\t= 0.341424 ~ 0\t(ideal: 0)\n"
     ]
    }
   ],
   "source": [
    "def print_result(x1, x2):\n",
    "    result = calculate(x1, x2)\n",
    "    \n",
    "    ideal = (1 if x1 > 0.5 else 0) ^ (1 if x2 > 0.5 else 0)\n",
    "    print(f'{x1:.2f}\\txor\\t{x2:.2f}\\t= {result.item():.6f} ~ {int(np.floor(result.item()/0.75))}\\t(ideal: {ideal})')\n",
    "\n",
    "# numbers = [(-0.5, -0.5), \n",
    "#            (0.2, 0.2), \n",
    "#            (0.8, 0.8), \n",
    "#            (1.5, 1.5), \n",
    "#            (-0.3, 1.2), \n",
    "#            (-0.41457323560865483,1.0737424253012686),\n",
    "#            (-0.29388577555129003,1.139695911993421), \n",
    "#            (1.036596609796075,-0.0947945731447089),\n",
    "#            (0.16133601091769956,-0.3312406173692577),\n",
    "#            (-0.3786384832936716,-0.463855165487841),\n",
    "#            (1.2340509291499595,0.01902263779733182),\n",
    "#            (1.042442196056173,1.4429183074575107)\n",
    "#            ]\n",
    "numbers, y = generate_data(20) \n",
    "for x1, x2 in numbers:\n",
    "    print_result(x1, x2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
